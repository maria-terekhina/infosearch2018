{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      " * Restarting with stat\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\masha\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2855: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request, redirect, url_for\n",
    "import json\n",
    "import pickle\n",
    "import pymorphy2\n",
    "from judicial_splitter import splitter as sp\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import os\n",
    "from math import log\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "with open('inv_base.pkl', 'rb') as f:\n",
    "    inv_base = pickle.load(f)\n",
    "\n",
    "def preprocessing(input_text, del_stopwords=True, del_digit=True):\n",
    "    \"\"\"\n",
    "    :input: raw text\n",
    "        1. lowercase, del punctuation, tokenize\n",
    "        2. normal form\n",
    "        3. del stopwords\n",
    "        4. del digits\n",
    "    :return: lemmas\n",
    "    \"\"\"\n",
    "    russian_stopwords = set(stopwords.words('russian'))\n",
    "    words = [x.lower().strip(string.punctuation+'»«–…') for x in word_tokenize(input_text)]\n",
    "    lemmas = [morph.parse(x)[0].normal_form for x in words if x]\n",
    "\n",
    "    lemmas_arr = []\n",
    "    for lemma in lemmas:\n",
    "        if del_stopwords:\n",
    "            if lemma in russian_stopwords:\n",
    "                continue\n",
    "        if del_digit:\n",
    "            if lemma.isdigit():\n",
    "                continue\n",
    "        lemmas_arr.append(lemma)\n",
    "    return lemmas_arr\n",
    "\n",
    "class IterDocs(object):\n",
    "    def __init__(self, text=False, lemmas=False, tagged=False):\n",
    "        self.text = text\n",
    "        self.lemmas = lemmas\n",
    "        self.tagged = tagged\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for root, dirs, files in os.walk('./avito_parsed'):\n",
    "            for i, file in enumerate(files):\n",
    "                with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                    if self.tagged is True:\n",
    "                        yield TaggedDocument(words=json.load(f), tags=[i])\n",
    "                    elif self.text is True: \n",
    "                        yield ' '.join(json.load(f))\n",
    "                    elif self.lemmas is True:\n",
    "                        yield json.load(f)\n",
    "\n",
    "def score_BM25(qf, dl, avgdl, k1, b, N, n) -> float:\n",
    "    \"\"\"\n",
    "    Compute similarity score between search query and documents from collection\n",
    "    :return: score\n",
    "    \"\"\"\n",
    "    idf = log((N - n + 0.5) / (n + 0.5))\n",
    "    score = (idf * (k1 + 1) * qf) / (qf + k1 * (1 - b + b * dl / avgdl))\n",
    "        \n",
    "    return score\n",
    "\n",
    "def compute_sim(query, doc, inv_index, k1, b, avgdl, N) -> float:\n",
    "    \"\"\"\n",
    "    Compute parameters for BM25 score and pass them to the calculation function\n",
    "    :param query: str: word for which to claculate BM25\n",
    "    :param doc: str: doc for which to claculate BM25\n",
    "    :param inv_index: default_dict: inverted index for the collection, that includes doc\n",
    "    :return: score\n",
    "    \"\"\"\n",
    "    qf = doc.count(query)\n",
    "    dl = len(doc)\n",
    "    \n",
    "    if query in inv_index:\n",
    "        n = len(inv_index[query])\n",
    "    else:\n",
    "        n = 0\n",
    "     \n",
    "    return score_BM25(qf, dl, avgdl, k1, b, N, n)\n",
    "\n",
    "def search_inv(query, corpus, inv_index) -> list:\n",
    "    \"\"\"\n",
    "    Search documents relative to query using inverted index algorithm.\n",
    "    :param query: str: input text\n",
    "    :param questions: list: all questions from corpus\n",
    "    :param answers: list: all answers from corpus\n",
    "    :param inv_index: list: questions inverted index\n",
    "    :return: list: 5 relevant answers\n",
    "    \"\"\"\n",
    "    def mean(numbers):\n",
    "        return float(sum(numbers)) / max(len(numbers), 1)\n",
    "\n",
    "    k1 = 2.0\n",
    "    b = 0.75\n",
    "    file_lens = [len(file) for file in IterDocs(lemmas=True)]\n",
    "    avgdl = mean(file_lens)\n",
    "    N = len(file_lens)\n",
    "\n",
    "    \n",
    "    query_list = preprocessing(query)\n",
    "    scores = list()\n",
    "    \n",
    "    for i, doc in enumerate(corpus):\n",
    "        score = 0\n",
    "        for word in query_list:\n",
    "            score += compute_sim(word, doc, inv_index, k1, b, avgdl, N)\n",
    "        scores.append([i, score])\n",
    "        \n",
    "    ranked = sorted(scores, key = lambda x: x[1], reverse=True)\n",
    "    \n",
    "    result = list()\n",
    "    names = list()\n",
    "    i = 0\n",
    "    while len(result) < 5:\n",
    "        doc = ranked[i]\n",
    "        name = os.listdir('./avito_parsed')[doc[0]][:-7]\n",
    "        if name[-1] is '_':\n",
    "            name = name[:-1]\n",
    "        name += '.txt'\n",
    "\n",
    "        if not name in names:\n",
    "            names.append(name)            \n",
    "            with open('./avito_texts/%s' %(name), 'r', encoding='utf-8') as f:\n",
    "                result.append(f.read())\n",
    "        i += 1\n",
    "\n",
    "    return result\n",
    "\n",
    "def search(query, search_method):\n",
    "    if search_method == 'inverted_index':\n",
    "        search_result = search_inv(query, IterDocs(lemmas=True), inv_base)\n",
    "    elif search_method == 'word2vec':\n",
    "        search_result = search_w2v(query, w2v_base)\n",
    "    elif search_method == 'doc2vec':\n",
    "        search_result = search_d2v(query, d2v_base)\n",
    "    else:\n",
    "        raise TypeError('unsupported search method')\n",
    "    return search_result\n",
    "\n",
    "@app.route('/')\n",
    "def search_fucntion():\n",
    "    if request.args:\n",
    "        query = rquest.args['query']\n",
    "        return render_template('results.html')\n",
    "    else:\n",
    "        return render_template('index.html')\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    app.run(debug = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-97620ec0e159>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m     \u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdebug\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\masha\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\flask\\app.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, host, port, debug, **options)\u001b[0m\n\u001b[0;32m    841\u001b[0m         \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'passthrough_errors'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 843\u001b[1;33m             \u001b[0mrun_simple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    844\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    845\u001b[0m             \u001b[1;31m# reset the first request information if the development server\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\masha\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\werkzeug\\serving.py\u001b[0m in \u001b[0;36mrun_simple\u001b[1;34m(hostname, port, application, use_reloader, use_debugger, use_evalex, extra_files, reloader_interval, reloader_type, threaded, processes, request_handler, static_files, passthrough_errors, ssl_context)\u001b[0m\n\u001b[0;32m    690\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_reloader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrun_with_reloader\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m         run_with_reloader(inner, extra_files, reloader_interval,\n\u001b[1;32m--> 692\u001b[1;33m                           reloader_type)\n\u001b[0m\u001b[0;32m    693\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m         \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\masha\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\werkzeug\\_reloader.py\u001b[0m in \u001b[0;36mrun_with_reloader\u001b[1;34m(main_func, extra_files, interval, reloader_type)\u001b[0m\n\u001b[0;32m    248\u001b[0m             \u001b[0mreloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m             \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestart_with_reloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemExit\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "with open('inv_base.pkl', 'rb') as f:\n",
    "    inv_base = pickle.load(f)\n",
    "\n",
    "def preprocessing(input_text, del_stopwords=True, del_digit=True):\n",
    "    \"\"\"\n",
    "    :input: raw text\n",
    "        1. lowercase, del punctuation, tokenize\n",
    "        2. normal form\n",
    "        3. del stopwords\n",
    "        4. del digits\n",
    "    :return: lemmas\n",
    "    \"\"\"\n",
    "    russian_stopwords = set(stopwords.words('russian'))\n",
    "    words = [x.lower().strip(string.punctuation+'»«–…') for x in word_tokenize(input_text)]\n",
    "    lemmas = [morph.parse(x)[0].normal_form for x in words if x]\n",
    "\n",
    "    lemmas_arr = []\n",
    "    for lemma in lemmas:\n",
    "        if del_stopwords:\n",
    "            if lemma in russian_stopwords:\n",
    "                continue\n",
    "        if del_digit:\n",
    "            if lemma.isdigit():\n",
    "                continue\n",
    "        lemmas_arr.append(lemma)\n",
    "    return lemmas_arr\n",
    "\n",
    "class IterDocs(object):\n",
    "    def __init__(self, text=False, lemmas=False, tagged=False):\n",
    "        self.text = text\n",
    "        self.lemmas = lemmas\n",
    "        self.tagged = tagged\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for root, dirs, files in os.walk('./avito_parsed'):\n",
    "            for i, file in enumerate(files):\n",
    "                with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                    if self.tagged is True:\n",
    "                        yield TaggedDocument(words=json.load(f), tags=[i])\n",
    "                    elif self.text is True: \n",
    "                        yield ' '.join(json.load(f))\n",
    "                    elif self.lemmas is True:\n",
    "                        yield json.load(f)\n",
    "\n",
    "def score_BM25(qf, dl, avgdl, k1, b, N, n) -> float:\n",
    "    \"\"\"\n",
    "    Compute similarity score between search query and documents from collection\n",
    "    :return: score\n",
    "    \"\"\"\n",
    "    idf = log((N - n + 0.5) / (n + 0.5))\n",
    "    score = (idf * (k1 + 1) * qf) / (qf + k1 * (1 - b + b * dl / avgdl))\n",
    "        \n",
    "    return score\n",
    "\n",
    "def compute_sim(query, doc, inv_index, k1, b, avgdl, N) -> float:\n",
    "    \"\"\"\n",
    "    Compute parameters for BM25 score and pass them to the calculation function\n",
    "    :param query: str: word for which to claculate BM25\n",
    "    :param doc: str: doc for which to claculate BM25\n",
    "    :param inv_index: default_dict: inverted index for the collection, that includes doc\n",
    "    :return: score\n",
    "    \"\"\"\n",
    "    qf = doc.count(query)\n",
    "    dl = len(doc)\n",
    "    \n",
    "    if query in inv_index:\n",
    "        n = len(inv_index[query])\n",
    "    else:\n",
    "        n = 0\n",
    "     \n",
    "    return score_BM25(qf, dl, avgdl, k1, b, N, n)\n",
    "\n",
    "def search_inv(query, corpus, inv_index) -> list:\n",
    "    \"\"\"\n",
    "    Search documents relative to query using inverted index algorithm.\n",
    "    :param query: str: input text\n",
    "    :param questions: list: all questions from corpus\n",
    "    :param answers: list: all answers from corpus\n",
    "    :param inv_index: list: questions inverted index\n",
    "    :return: list: 5 relevant answers\n",
    "    \"\"\"\n",
    "    def mean(numbers):\n",
    "        return float(sum(numbers)) / max(len(numbers), 1)\n",
    "\n",
    "    k1 = 2.0\n",
    "    b = 0.75\n",
    "    file_lens = [len(file) for file in IterDocs(lemmas=True)]\n",
    "    avgdl = mean(file_lens)\n",
    "    N = len(file_lens)\n",
    "\n",
    "    \n",
    "    query_list = preprocessing(query)\n",
    "    scores = list()\n",
    "    \n",
    "    for i, doc in enumerate(corpus):\n",
    "        score = 0\n",
    "        for word in query_list:\n",
    "            score += compute_sim(word, doc, inv_index, k1, b, avgdl, N)\n",
    "        scores.append([i, score])\n",
    "        \n",
    "    ranked = sorted(scores, key = lambda x: x[1], reverse=True)\n",
    "    \n",
    "    result = list()\n",
    "    names = list()\n",
    "    i = 0\n",
    "    while len(result) < 5:\n",
    "        doc = ranked[i]\n",
    "        name = os.listdir('./avito_parsed')[doc[0]][:-7]\n",
    "        if name[-1] is '_':\n",
    "            name = name[:-1]\n",
    "        name += '.txt'\n",
    "\n",
    "        if not name in names:\n",
    "            names.append(name)            \n",
    "            with open('./avito_texts/%s' %(name), 'r', encoding='utf-8') as f:\n",
    "                result.append(f.read())\n",
    "        i += 1\n",
    "\n",
    "    return result\n",
    "\n",
    "def search(query, search_method):\n",
    "    if search_method == 'inverted_index':\n",
    "        search_result = search_inv(query, IterDocs(lemmas=True), inv_base)\n",
    "    elif search_method == 'word2vec':\n",
    "        search_result = search_w2v(query, w2v_base)\n",
    "    elif search_method == 'doc2vec':\n",
    "        search_result = search_d2v(query, d2v_base)\n",
    "    else:\n",
    "        raise TypeError('unsupported search method')\n",
    "    return search_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
