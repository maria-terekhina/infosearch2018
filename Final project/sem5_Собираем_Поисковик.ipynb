{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар 5    \n",
    "## Собираем поисковик \n",
    "\n",
    "![](https://bilimfili.com/wp-content/uploads/2017/06/bir-urune-emek-vermek-o-urune-olan-deger-algimizi-degistirir-mi-bilimfilicom.jpg) \n",
    "\n",
    "\n",
    "Мы уже все знаем, для того чтобы сделать поисковик. Осталось соединить все части вместе.    \n",
    "Итак, для поисковика нам понадобятся:         \n",
    "**1. База документов **\n",
    "> в первом дз - корпус Друзей    \n",
    "в сегодняшнем дз - корпус юридических вопросов-ответов    \n",
    "в итоговом проекте - корпус Авито   \n",
    "\n",
    "**2. Функция индексации**                 \n",
    "Что делает: собирает информацию о корпусе, по которуму будет происходить поиск      \n",
    "Своя для каждого поискового метода:       \n",
    "> A. для обратного индекса она создает обратный индекс (чудо) и сохраняет статистики корпуса, необходимые для Okapi BM25 (средняя длина документа в коллекции, количество доков ... )             \n",
    "> B. для поиска через word2vec эта функция создает вектор для каждого документа в коллекции путем, например, усреднения всех векторов коллекции       \n",
    "> C. для поиска через doc2vec эта функция создает вектор для каждого документа               \n",
    "\n",
    "   Не забывайте сохранить все, что насчитает эта функция. Если это будет происходить налету во время поиска, понятно, что он будет работать сто лет     \n",
    "   \n",
    "**3. Функция поиска**     \n",
    "Можно разделить на две части:\n",
    "1. функция вычисления близости между запросом и документом    \n",
    "> 1. для индекса это Okapi BM25\n",
    "> 2. для w2v и d2v это обычная косинусная близость между векторами          \n",
    "2. ранжирование (или просто сортировка)\n",
    "\n",
    "\n",
    "Время все это реализовать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\masha\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pymorphy2\n",
    "import os\n",
    "from tqdm import tqdm_notebook\n",
    "import json\n",
    "import numpy as np\n",
    "from judicial_splitter import splitter as sp\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from math import log\n",
    "from gensim import matutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Препроцессинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def preprocessing(input_text, del_stopwords=True, del_digit=True):\n",
    "    \"\"\"\n",
    "    :input: raw text\n",
    "        1. lowercase, del punctuation, tokenize\n",
    "        2. normal form\n",
    "        3. del stopwords\n",
    "        4. del digits\n",
    "    :return: lemmas\n",
    "    \"\"\"\n",
    "    russian_stopwords = set(stopwords.words('russian'))\n",
    "    words = [x.lower().strip(string.punctuation+'»«–…') for x in word_tokenize(input_text)]\n",
    "    lemmas = [morph.parse(x)[0].normal_form for x in words if x]\n",
    "\n",
    "    lemmas_arr = []\n",
    "    for lemma in lemmas:\n",
    "        if del_stopwords:\n",
    "            if lemma in russian_stopwords:\n",
    "                continue\n",
    "        if del_digit:\n",
    "            if lemma.isdigit():\n",
    "                continue\n",
    "        lemmas_arr.append(lemma)\n",
    "    return lemmas_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_doc(text, name, i):\n",
    "    '''\n",
    "    Write down item's metadata as .txt file.\n",
    "    text: str: metadata to be wtitten\n",
    "    url: str: url of an item\n",
    "    return: \n",
    "    '''\n",
    "    if not os.path.exists('./avito_parsed'):\n",
    "        os.makedirs('./avito_parsed')\n",
    "    with open(r'./avito_parsed/%s_%d.json' %(name[:-4], i), 'w', encoding='utf-8') as f:\n",
    "        json.dump(text, f, ensure_ascii=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "138d68ef0fef489c9e6764a27a825d28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8544), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for root, dirs, files in os.walk('./avito_texts'):\n",
    "    for file in tqdm_notebook(files):\n",
    "        with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            parts = sp(text, 3)\n",
    "            for i, p in enumerate(parts):\n",
    "                lemmas = preprocessing(p)\n",
    "                write_doc(lemmas, file, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Итаретор по документам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IterDocs(object):\n",
    "    def __init__(self, text=False, lemmas=False, tagged=False):\n",
    "        self.text = text\n",
    "        self.lemmas = lemmas\n",
    "        self.tagged = tagged\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for root, dirs, files in os.walk('./avito_parsed'):\n",
    "            for i, file in enumerate(files):\n",
    "                with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                    if self.tagged is True:\n",
    "                        yield TaggedDocument(words=json.load(f), tags=[i])\n",
    "                    elif self.text is True: \n",
    "                        yield ' '.join(json.load(f))\n",
    "                    elif self.lemmas is True:\n",
    "                        yield json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Индексация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обратный индекс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CV = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_inverted_index_base(corpus, names) -> dict:\n",
    "    \"\"\"\n",
    "    Create inverted index by input doc collection\n",
    "    :param corpus: list: input doc collection\n",
    "    :param names: list: list of names for input doc collection\n",
    "    :return: inverted index\n",
    "    \"\"\"\n",
    "    avito_vec = CV.fit_transform(corpus)\n",
    "    avito_df = pd.DataFrame(avito_vec.toarray(), columns=CV.get_feature_names())\n",
    "    \n",
    "    index = defaultdict()\n",
    "    \n",
    "    for col in tqdm_notebook(avito_df):\n",
    "        index[col] = [names[i] for i in list(avito_df[col][avito_df[col] > 0].index)]\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e363c2194e437b9d4475aa30217d6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11580), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# сохраняем обратный индекс\n",
    "inv_base = save_inverted_index_base(IterDocs(text=True), os.listdir('./avito_parsed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_BM25(qf, dl, avgdl, k1, b, N, n) -> float:\n",
    "    \"\"\"\n",
    "    Compute similarity score between search query and documents from collection\n",
    "    :return: score\n",
    "    \"\"\"\n",
    "    idf = log((N - n + 0.5) / (n + 0.5))\n",
    "    score = (idf * (k1 + 1) * qf) / (qf + k1 * (1 - b + b * dl / avgdl))\n",
    "        \n",
    "    return score\n",
    "\n",
    "def compute_sim(query, doc, inv_index, k1, b, avgdl, N) -> float:\n",
    "    \"\"\"\n",
    "    Compute parameters for BM25 score and pass them to the calculation function\n",
    "    :param query: str: word for which to claculate BM25\n",
    "    :param doc: str: doc for which to claculate BM25\n",
    "    :param inv_index: default_dict: inverted index for the collection, that includes doc\n",
    "    :return: score\n",
    "    \"\"\"\n",
    "    qf = doc.count(query)\n",
    "    dl = len(doc)\n",
    "    \n",
    "    if query in inv_index:\n",
    "        n = len(inv_index[query])\n",
    "    else:\n",
    "        n = 0\n",
    "     \n",
    "    return score_BM25(qf, dl, avgdl, k1, b, N, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "### Задание 1\n",
    "Загрузите любую понравившуюся вам word2vec модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_w2v = Word2Vec.load(r'araneum_none_fasttextskipgram_300_5_2018/araneum_none_fasttextskipgram_300_5_2018.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2 \n",
    "Напишите функцию индексации для поиска через word2vec. Она должна для каждого документа из корпуса строить вектор.   \n",
    "Все вектора надо сохранить, по формату советую json. При сохранении не забывайте, что вам надо сохранить не только  вектор, но и опознователь текста, которому он принадлежит. \n",
    "Для поисковика это может быть url страницы, для поиска по текстовому корпусу сам текст.\n",
    "\n",
    "> В качестве документа для word2vec берите **параграфы** исходного текста, а не весь текст целиком. Так вектора будут более осмысленными. В противном случае можно получить один очень общий вектор, релевантый совершенно разным запросам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_w2v_vectors(doc):\n",
    "    \"\"\"\n",
    "    Get doc w2v vector\n",
    "    :param doc: str: doc for which to compute vector\n",
    "    :return: list: cpmputed vector\n",
    "    \"\"\"\n",
    "    all_vect = list()\n",
    "    for word in doc:\n",
    "        try:\n",
    "            all_vect.append(model_w2v.wv[word])\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    if len(all_vect) != 0:\n",
    "        d_vect = np.mean(np.array(all_vect), axis=0)\n",
    "    else:\n",
    "        d_vect = [0]*300\n",
    "        \n",
    "    return d_vect \n",
    "\n",
    "def save_w2v_base(texts, idx):\n",
    "    \"\"\"\n",
    "    Save vectors for all passed documents\n",
    "    :param texts: list: documents of collection \n",
    "    :param idx: list: names of documants from collection \n",
    "    :return: list: list of vectors for input text\n",
    "    \"\"\"\n",
    "    base = list()\n",
    "    for i, doc in tqdm_notebook(enumerate(texts)):\n",
    "        base.append({'id': idx[i], 'text': doc, 'vec': list(get_w2v_vectors(doc))})\n",
    "    return base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "121743fc51124a6b85ec5cf0d49112fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-33:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\masha\\appdata\\local\\programs\\python\\python35\\lib\\threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\users\\masha\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tqdm\\_monitor.py\", line 62, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"c:\\users\\masha\\appdata\\local\\programs\\python\\python35\\lib\\_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# сохраняем w2v базу для вопросов и ответов\n",
    "w2v_base = save_w2v_base(IterDocs(lemmas=True), os.listdir('./avito_parsed'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec\n",
    "### Задание 3\n",
    "Напишите функцию обучения doc2vec на юридических текстах, и получите свою кастомную d2v модель. \n",
    "> Совет: есть мнение, что для обучения doc2vec модели не нужно удалять стоп-слова из корпуса. Они являются важными семантическими элементами.      \n",
    "\n",
    "Важно! В качестве документа для doc2vec берите **параграфы** исходного текста, а не весь текст целиком. И не забывайте про предобработку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_doc2vec(corpus, names):\n",
    "    '''\n",
    "    Train custome d2v model\n",
    "    :param names: pathes of the train data docs\n",
    "    :return: d2v model\n",
    "    '''\n",
    "    \n",
    "    model_d2v = Doc2Vec(vector_size=100, min_count=5, alpha=0.025, min_alpha=0.025, epochs=200, workers=4, dm=1, seed=42)\n",
    "    %time model_d2v.build_vocab(corpus)\n",
    "    print(len(model_d2v.wv.vocab))\n",
    "    \n",
    "    %time model_d2v.train(IterDocs(tagged=True), total_examples=model_d2v.corpus_count, epochs=model_d2v.epochs, report_delay=60)\n",
    "    return model_d2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.85 s\n",
      "7790\n",
      "Wall time: 29min 34s\n"
     ]
    }
   ],
   "source": [
    "model_d2v = train_doc2vec(IterDocs(tagged=True), os.listdir('./avito_parsed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = get_tmpfile(\"doc2vec_model_avito.model\")\n",
    "model_d2v.save(\"doc2vec_model_avito.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4\n",
    "Напишите функцию индексации для поиска через doc2vec. Она должна для каждого документа из корпуса получать вектор.    \n",
    "Все вектора надо сохранить, по формату советую json. При сохранении не забывайте, что вам надо сохранить не только вектор, но и опознователь текста, которому он принадлежит. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_d2v_vectors(words):\n",
    "    '''\n",
    "    Compute d2v vector for doc\n",
    "    :param words: list: lemmas of the doc for which to compute d2v vector\n",
    "    :return: list: d2v vector\n",
    "    '''\n",
    "    vec = model_d2v.infer_vector(words)\n",
    "    return vec \n",
    "\n",
    "def save_d2v_base(corpus, idx):\n",
    "    \"\"\"\n",
    "    Save d2v vectors for all passed documents\n",
    "    :param texts: list: documents of collection \n",
    "    :param idx: list: names of documants from collection \n",
    "    :return: list: list of d2v vectors for input text\n",
    "    \"\"\"\n",
    "    base = list()\n",
    "    for i, doc in tqdm_notebook(enumerate(corpus)):\n",
    "        base.append({'id': idx[i], 'text': doc, 'vect': list(get_d2v_vectors(doc))})\n",
    "    return base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a191bc39ecdf4ec7b55c398f57d0c9a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# сохраняем d2v базу \n",
    "d2v_base = save_d2v_base(IterDocs(lemmas=True), os.listdir('./avito_parsed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('inv_base.pkl', 'wb') as f:\n",
    "    pickle.dump(inv_base, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('d2v_base.pkl', 'wb') as f:\n",
    "    pickle.dump(d2v_base, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('w2v_base.pkl', 'wb') as f:\n",
    "    pickle.dump(w2v_base, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функция поиска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обратного индекса функцией поиска является Okapi BM25. Она у вас уже должна быть реализована."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция измерения близости между векторами нам пригодится:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similarity(v1, v2):\n",
    "    '''\n",
    "    Compute cosine similarity for 2 vectors\n",
    "    :param v1, v2: list: vectors\n",
    "    :return: float: vectors' similarity\n",
    "    '''\n",
    "    v1_norm = matutils.unitvec(np.array(v1))\n",
    "    v2_norm = matutils.unitvec(np.array(v2))\n",
    "    sim = np.dot(v1_norm, v2_norm)\n",
    "    if sim is not None:\n",
    "        return sim\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5\n",
    "Напишите функцию для поиска через word2vec и для поиска через doc2vec, которая по входящему запросу выдает отсортированную выдачу документов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_inv(query, corpus, inv_index) -> list:\n",
    "    \"\"\"\n",
    "    Search documents relative to query using inverted index algorithm.\n",
    "    :param query: str: input text\n",
    "    :param questions: list: all questions from corpus\n",
    "    :param answers: list: all answers from corpus\n",
    "    :param inv_index: list: questions inverted index\n",
    "    :return: list: 5 relevant answers\n",
    "    \"\"\"\n",
    "    k1 = 2.0\n",
    "    b = 0.75\n",
    "    file_lens = [len(file) for file in IterDocs(lemmas=True)]\n",
    "    avgdl = np.mean(file_lens)\n",
    "    N = len(file_lens)\n",
    "\n",
    "    \n",
    "    query_list = preprocessing(query)\n",
    "    scores = list()\n",
    "    \n",
    "    for i, doc in enumerate(corpus):\n",
    "        score = 0\n",
    "        for word in query_list:\n",
    "            score += compute_sim(word, doc, inv_index, k1, b, avgdl, N)\n",
    "        scores.append([i, score])\n",
    "        \n",
    "    ranked = sorted(scores, key = lambda x: x[1], reverse=True)\n",
    "    \n",
    "    result = list()\n",
    "    names = list()\n",
    "    i = 0\n",
    "    while len(result) < 5:\n",
    "        doc = ranked[i]\n",
    "        name = os.listdir('./avito_parsed')[doc[0]][:-7]\n",
    "        if name[-1] is '_':\n",
    "            name = name[:-1]\n",
    "        name += '.txt'\n",
    "\n",
    "        if not name in names:\n",
    "            names.append(name)            \n",
    "            with open('./avito_texts/%s' %(name), 'r', encoding='utf-8') as f:\n",
    "                result.append(f.read())\n",
    "        i += 1\n",
    "\n",
    "    return result\n",
    "\n",
    "def search_w2v(query, w2v_base) -> list:\n",
    "    \"\"\"\n",
    "    Search documents relative to query using inverted w2v algorithm.\n",
    "    :param query: str: input text\n",
    "    :param w2v_base_quest: list: all questions' vectors from corpus\n",
    "    :param answers: list: all answers from corpus\n",
    "    :return: list: 5 relative answers\n",
    "    \"\"\"\n",
    "    \n",
    "    similarities = list()\n",
    "\n",
    "    for part in sp(query, 3):\n",
    "        lemmas = preprocessing(query)\n",
    "        vec = get_w2v_vectors(lemmas)\n",
    "    \n",
    "        for doc in w2v_base:\n",
    "            s = similarity(vec, doc['vec'])\n",
    "            similarities.append({'id': doc['id'], 'sim': s})\n",
    "\n",
    "    ranked = sorted(similarities, key=lambda x: x['sim'], reverse=True)\n",
    "    \n",
    "    result = list()\n",
    "    names = list()\n",
    "    i = 0\n",
    "    while len(result) < 5:\n",
    "        doc = ranked[i]\n",
    "        name = doc['id'][:-7]\n",
    "        if name[-1] is '_':\n",
    "            name = name[:-1]\n",
    "        name += '.txt'\n",
    "\n",
    "        if not name in names:\n",
    "            names.append(name)            \n",
    "            with open('./avito_texts/%s' %(name), 'r', encoding='utf-8') as f:\n",
    "                result.append(f.read())\n",
    "        i += 1\n",
    "    \n",
    "    return result\n",
    "    \n",
    "def search_d2v(query, d2v_base) -> list:\n",
    "    \"\"\"\n",
    "    Search documents relative to query using inverted d2v algorithm.\n",
    "    :param query: str: input text\n",
    "    :param d2v_base_quest: list: all questions' vectors from corpus\n",
    "    :param answers: list: all answers from corpus\n",
    "    :return: list: 5 relative answers\n",
    "    \"\"\"\n",
    "    similarities = list()\n",
    "\n",
    "    for part in sp(query, 3):\n",
    "        lemmas = preprocessing(query)\n",
    "        vec = get_d2v_vectors(lemmas)\n",
    "    \n",
    "        for doc in d2v_base:\n",
    "            s = similarity(vec, doc['vect'])\n",
    "            similarities.append({'id': doc['id'], 'sim': s})\n",
    "\n",
    "    ranked = sorted(similarities, key=lambda x: x['sim'], reverse=True)\n",
    "    \n",
    "    result = list()\n",
    "    names = list()\n",
    "    i = 0\n",
    "    while len(result) < 5:\n",
    "        doc = ranked[i]\n",
    "        name = doc['id'][:-7]\n",
    "        if name[-1] is '_':\n",
    "            name = name[:-1]\n",
    "        name += '.txt'\n",
    "\n",
    "        if not name in names:\n",
    "            names.append(name)            \n",
    "            with open('./avito_texts/%s' %(name), 'r', encoding='utf-8') as f:\n",
    "                result.append(f.read())\n",
    "        i += 1\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После выполнения всех этих заданий ваш поисковик готов, поздравляю!                  \n",
    "Осталось завернуть все написанное в питон скрипт, и сделать общую функцию поиска гибким, чтобы мы могли искать как по обратному индексу, так и по word2vec, так и по doc2vec.          \n",
    "Сделать это можно очень просто через старый добрый ``` if ```, который будет дергать ту или иную функцию поиска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search(query, search_method):\n",
    "    if search_method == 'inverted_index':\n",
    "        search_result = search_inv(query, IterDocs(lemmas=True), inv_base)\n",
    "    elif search_method == 'word2vec':\n",
    "        search_result = search_w2v(query, w2v_base)\n",
    "    elif search_method == 'doc2vec':\n",
    "        search_result = search_d2v(query, d2v_base)\n",
    "    else:\n",
    "        raise TypeError('unsupported search method')\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.avito.ru/moskva/bilety_i_puteshestviya/bilety_v_bolshoy_teatr_na_spektakli_1521091832\n",
      "Билеты в Большой театр на спектакли купить в Москве на Avito — Объявления на сайте Авито\n",
      "№ 1521091832, размещено 8 октября в 14:49\n",
      "ОКТЯБРЬ:ИСТОРИЧЕСКАЯ СЦЕНАЧисло:9,10.              Жизель (балет) 12,13.            Анна Карен на (балет) 14.                 Реквием (концерт) 16,17.            Драгоценности (балет) 20,21.            Нуриев (балет) 23,24.            Травиата (опера) 25,27,28.       Спартак (балет) 26.                 Лебединое озеро (балет) 30,31.            Драгоценности (балет) НОВАЯ СЦЕНА Число:9,10,11.          Так поступают все женщины 17,19,21,23.   Альбина (опера) 18,20,21.        Путеводитель по аркестру                        (театрализованный концерт) Цены на билеты разные, все зависит от места. Звоните! Подберём лучшие места для Вас!Доставка по Москве!Если интересуют другие спектакли в Большом театре, то можете обращаться!\n",
      "Москва\n",
      "м. Театральная\n",
      "https://www.avito.ru/kazan/bilety_i_puteshestviya/balet_spyaschaya_krasavitsa_28.10_1071751560\n",
      "Балет Спящая красавица 28.10\n",
      "№ 1071751560, размещено сегодня в 00:30\n",
      "ТЕАТР ОПЕРЫ И БАЛЕТА партер 2 билета\n",
      "Казань\n",
      "м. Козья Слобода\n",
      "https://www.avito.ru/sankt-peterburg/bilety_i_puteshestviya/mariinskiy_teatr._vecher_odnoaktnyh_baletov_11.10_1633744168\n",
      "Мариинский театр. Вечер одноактных балетов 11.10\n",
      "№ 1633744168, размещено вчера в 20:13\n",
      "Продам   два билета  на  балеты Блудный сын. Скрипичный концерт №2. Русская увертюра в Мариинском театре 11.10  Партер по 850\n",
      "850\n",
      "Санкт-Петербург\n",
      "м. Беговая\n",
      "https://www.avito.ru/moskva/bilety_i_puteshestviya/bilety_na_balet_spartak_969842398\n",
      "Билеты на балет \"Спартак\"\n",
      "№ 969842398, размещено 17 октября в 11:49\n",
      "Билеты на балет в большой театр на Историческую сцену на балет \"Спартак\" 25 26 и 27 октября.\n",
      "2 200\n",
      "Москва\n",
      "м. Крестьянская застава\n",
      "https://www.avito.ru/petrozavodsk/bilety_i_puteshestviya/bilet_na_balet_1284732362\n",
      "Билет на балет\n",
      "№ 1284732362, размещено 17 октября в 09:17\n",
      "Продам билет на балет \"Дон Кихот\" на 17 октября на 19.00, в ложе бенуара, в Музыкальном театре.\n",
      "325\n",
      "Петрозаводск\n",
      "р-н Центр\n"
     ]
    }
   ],
   "source": [
    "for r in search('балет', 'doc2vec'):\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, request, redirect, url_for\n",
    "import json\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "   \n",
    "@app.route('/')\n",
    "def search_fucntion():\n",
    "    if request.args:\n",
    "        method = rquest.args['search_method']\n",
    "        query = rquest.args['query']\n",
    "        return render_template('results.html')\n",
    "    else:\n",
    "        return render_template('main.html')\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    app.run(debug = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, request, redirect, url_for\n",
    "import json\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "   \n",
    "@app.route('/')\n",
    "def search_fucntion():\n",
    "    if request.args:\n",
    "        method = rquest.args['search_method']\n",
    "        query = rquest.args['query']\n",
    "        return render_template('results.html')\n",
    "    else:\n",
    "        return render_template('main.html')\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    app.run(debug = True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
