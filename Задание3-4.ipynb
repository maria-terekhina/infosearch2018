{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар 5    \n",
    "## Собираем поисковик \n",
    "\n",
    "![](https://bilimfili.com/wp-content/uploads/2017/06/bir-urune-emek-vermek-o-urune-olan-deger-algimizi-degistirir-mi-bilimfilicom.jpg) \n",
    "\n",
    "\n",
    "Мы уже все знаем, для того чтобы сделать поисковик. Осталось соединить все части вместе.    \n",
    "Итак, для поисковика нам понадобятся:         \n",
    "**1. База документов **\n",
    "> в первом дз - корпус Друзей    \n",
    "в сегодняшнем дз - корпус юридических вопросов-ответов    \n",
    "в итоговом проекте - корпус Авито   \n",
    "\n",
    "**2. Функция индексации**                 \n",
    "Что делает: собирает информацию о корпусе, по которуму будет происходить поиск      \n",
    "Своя для каждого поискового метода:       \n",
    "> A. для обратного индекса она создает обратный индекс (чудо) и сохраняет статистики корпуса, необходимые для Okapi BM25 (средняя длина документа в коллекции, количество доков ... )             \n",
    "> B. для поиска через word2vec эта функция создает вектор для каждого документа в коллекции путем, например, усреднения всех векторов коллекции       \n",
    "> C. для поиска через doc2vec эта функция создает вектор для каждого документа               \n",
    "\n",
    "   Не забывайте сохранить все, что насчитает эта функция. Если это будет происходить налету во время поиска, понятно, что он будет работать сто лет     \n",
    "   \n",
    "**3. Функция поиска**     \n",
    "Можно разделить на две части:\n",
    "1. функция вычисления близости между запросом и документом    \n",
    "> 1. для индекса это Okapi BM25\n",
    "> 2. для w2v и d2v это обычная косинусная близость между векторами          \n",
    "2. ранжирование (или просто сортировка)\n",
    "\n",
    "\n",
    "Время все это реализовать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1384"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('qa_corpus.pkl', 'rb') as file:\n",
    "    qa_corpus = pickle.load(file)\n",
    "    \n",
    "len(qa_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "questions = list()\n",
    "answers = list()\n",
    "\n",
    "for item in qa_corpus:\n",
    "    questions.append(item[0])\n",
    "    answers.append(item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Препроцессинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def preprocessing(input_text, del_stopwords=True, del_digit=True):\n",
    "    \"\"\"\n",
    "    :input: raw text\n",
    "        1. lowercase, del punctuation, tokenize\n",
    "        2. normal form\n",
    "        3. del stopwords\n",
    "        4. del digits\n",
    "    :return: lemmas\n",
    "    \"\"\"\n",
    "    russian_stopwords = set(stopwords.words('russian'))\n",
    "    words = [x.lower().strip(string.punctuation+'»«–…') for x in word_tokenize(input_text)]\n",
    "    lemmas = [morph.parse(x)[0].normal_form for x in words if x]\n",
    "\n",
    "    lemmas_arr = []\n",
    "    for lemma in lemmas:\n",
    "        if del_stopwords:\n",
    "            if lemma in russian_stopwords:\n",
    "                continue\n",
    "        if del_digit:\n",
    "            if lemma.isdigit():\n",
    "                continue\n",
    "        lemmas_arr.append(lemma)\n",
    "    return lemmas_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Индексация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from judicial_splitter import splitter as sp\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from math import log\n",
    "from gensim import matutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обратный индекс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1384/1384 [05:23<00:00,  4.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# собираем списки лемм для вопросов и ответов\n",
    "questions_lemmas = list()\n",
    "answers_lemmas = list()\n",
    "\n",
    "for pair in tqdm(qa_corpus):\n",
    "    questions_lemmas.append(preprocessing(pair[0]))\n",
    "    answers_lemmas.append(preprocessing(pair[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 1384/1384 [00:00<00:00, 62846.22it/s]\n",
      "100%|███████████████████████████████████| 1384/1384 [00:00<00:00, 44609.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# делаем из списков лемм строки для работы CountVectorizer\n",
    "questions_joint = list()\n",
    "answers_joint = list()\n",
    "\n",
    "for doc in tqdm(questions_lemmas):\n",
    "    questions_joint.append(' '.join(doc))\n",
    "    \n",
    "for doc in tqdm(answers_lemmas):\n",
    "    answers_joint.append(' '.join(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>0001</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>...</th>\n",
       "      <th>январялюдмилаи</th>\n",
       "      <th>ярлык</th>\n",
       "      <th>ярослав</th>\n",
       "      <th>ярославский</th>\n",
       "      <th>ясна</th>\n",
       "      <th>ясно</th>\n",
       "      <th>яхта</th>\n",
       "      <th>ячейка</th>\n",
       "      <th>ящик</th>\n",
       "      <th>ёрш</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8928 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  0001  01  02  03  04  05  06  07  08 ...   январялюдмилаи  ярлык  \\\n",
       "0   0     0   0   0   0   0   0   0   0   0 ...                0      0   \n",
       "1   0     0   0   0   0   0   0   0   0   0 ...                0      0   \n",
       "2   0     0   0   0   0   0   0   0   0   0 ...                0      0   \n",
       "3   0     0   0   0   0   0   0   0   0   0 ...                0      0   \n",
       "4   0     0   0   0   0   0   0   0   0   0 ...                0      0   \n",
       "\n",
       "   ярослав  ярославский  ясна  ясно  яхта  ячейка  ящик  ёрш  \n",
       "0        0            0     0     1     0       0     0    0  \n",
       "1        0            0     0     0     0       0     0    0  \n",
       "2        0            0     0     0     0       0     0    0  \n",
       "3        0            0     0     0     0       0     0    0  \n",
       "4        0            0     0     0     0       0     0    0  \n",
       "\n",
       "[5 rows x 8928 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# обучаем CountVectorizer\n",
    "CV = CountVectorizer()\n",
    "q_vec = CV.fit_transform(questions_joint)\n",
    "q_df = pd.DataFrame(q_vec.toarray(), columns=CV.get_feature_names())\n",
    "\n",
    "a_vec = CV.fit_transform(answers_joint)\n",
    "a_df = pd.DataFrame(a_vec.toarray(), columns=CV.get_feature_names())\n",
    "\n",
    "a_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_inverted_index_base(corpus, names) -> dict:\n",
    "    \"\"\"\n",
    "    Create inverted index by input doc collection\n",
    "    :param corpus: list: input doc collection\n",
    "    :param names: list: list of names for input doc collection\n",
    "    :return: inverted index\n",
    "    \"\"\"\n",
    "    joint_lemmas = list()\n",
    "\n",
    "    for doc in tqdm(corpus):\n",
    "        joint_lemmas.append(' '.join(preprocessing(doc)))\n",
    "\n",
    "    corpus_vec = CV.fit_transform(joint_lemmas)\n",
    "    corpus_df = pd.DataFrame(corpus_vec.toarray(), columns=CV.get_feature_names())\n",
    "    \n",
    "    index = defaultdict()\n",
    "    \n",
    "    for col in tqdm(corpus_df):\n",
    "        index[col] = [names[i] for i in list(corpus_df[col][corpus_df[col] > 0].index)]\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1384/1384 [01:54<00:00, 12.12it/s]\n",
      "7955it [00:21, 362.71it/s]                                                     \n",
      "100%|██████████████████████████████████████| 1384/1384 [04:01<00:00,  5.73it/s]\n",
      "8928it [00:24, 362.28it/s]                                                     \n"
     ]
    }
   ],
   "source": [
    "# сохраняем обратный индекс для вопросов и ответов\n",
    "inv_q_base = save_inverted_index_base(questions, list(range(0, len(questions))))\n",
    "inv_a_base = save_inverted_index_base(answers, list(range(0, len(answers))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_BM25(qf, dl, avgdl, k1, b, N, n) -> float:\n",
    "    \"\"\"\n",
    "    Compute similarity score between search query and documents from collection\n",
    "    :return: score\n",
    "    \"\"\"\n",
    "    idf = log((N - n + 0.5) / (n + 0.5))\n",
    "    score = (idf * (k1 + 1) * qf) / (qf + k1 * (1 - b + b * dl / avgdl))\n",
    "        \n",
    "    return score\n",
    "\n",
    "def compute_sim(query, doc, inv_index, k1, b, avgdl, N) -> float:\n",
    "    \"\"\"\n",
    "    Compute parameters for BM25 score and pass them to the calculation function\n",
    "    :param query: str: word for which to claculate BM25\n",
    "    :param doc: str: doc for which to claculate BM25\n",
    "    :param inv_index: default_dict: inverted index for the collection, that includes doc\n",
    "    :return: score\n",
    "    \"\"\"\n",
    "    qf = doc.count(query)\n",
    "    dl = len(doc)\n",
    "    \n",
    "    if query in inv_index:\n",
    "        n = len(inv_index[query])\n",
    "    else:\n",
    "        n = 0\n",
    "     \n",
    "    return score_BM25(qf, dl, avgdl, k1, b, N, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "### Задание 1\n",
    "Загрузите любую понравившуюся вам word2vec модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\masha\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_w2v = Word2Vec.load(r'araneum_none_fasttextskipgram_300_5_2018/araneum_none_fasttextskipgram_300_5_2018.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2 \n",
    "Напишите функцию индексации для поиска через word2vec. Она должна для каждого документа из корпуса строить вектор.   \n",
    "Все вектора надо сохранить, по формату советую json. При сохранении не забывайте, что вам надо сохранить не только  вектор, но и опознователь текста, которому он принадлежит. \n",
    "Для поисковика это может быть url страницы, для поиска по текстовому корпусу сам текст.\n",
    "\n",
    "> В качестве документа для word2vec берите **параграфы** исходного текста, а не весь текст целиком. Так вектора будут более осмысленными. В противном случае можно получить один очень общий вектор, релевантый совершенно разным запросам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_w2v_vectors(doc):\n",
    "    \"\"\"\n",
    "    Get doc w2v vector\n",
    "    :param doc: str: doc for which to compute vector\n",
    "    :return: list: cpmputed vector\n",
    "    \"\"\"\n",
    "    all_vect = list()\n",
    "    for word in doc:\n",
    "        try:\n",
    "            all_vect.append(model_w2v.wv[word])\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    if len(all_vect) != 0:\n",
    "        d_vect = np.mean(np.array(all_vect), axis=0)\n",
    "    else:\n",
    "        d_vect = [0]*300\n",
    "        \n",
    "    return d_vect \n",
    "\n",
    "def save_w2v_base(texts, idx):\n",
    "    \"\"\"\n",
    "    Save vectors for all passed documents\n",
    "    :param texts: list: documents of collection \n",
    "    :param idx: list: names of documants from collection \n",
    "    :return: list: list of vectors for input text\n",
    "    \"\"\"\n",
    "    base = list()\n",
    "    for i, doc in tqdm(enumerate(texts)):\n",
    "        parts = sp(doc, 3)\n",
    "        for p in parts:\n",
    "            lemmas = preprocessing(p)\n",
    "            base.append({'id': idx[i], 'text': p, 'vec': get_w2v_vectors(lemmas)})\n",
    "    return base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1384it [01:55, 12.02it/s]\n",
      "1384it [03:51,  5.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# сохраняем w2v базу для вопросов и ответов\n",
    "w2v_base_quest = save_w2v_base(questions, list(range(0, len(questions))))\n",
    "w2v_base_answ = save_w2v_base(answers, list(range(0, len(answers))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec\n",
    "### Задание 3\n",
    "Напишите функцию обучения doc2vec на юридических текстах, и получите свою кастомную d2v модель. \n",
    "> Совет: есть мнение, что для обучения doc2vec модели не нужно удалять стоп-слова из корпуса. Они являются важными семантическими элементами.      \n",
    "\n",
    "Важно! В качестве документа для doc2vec берите **параграфы** исходного текста, а не весь текст целиком. И не забывайте про предобработку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.utils import get_tmpfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 224756/224756 [00:03<00:00, 58841.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# собираем пути документов коллекции\n",
    "names = list()\n",
    "for root, dirs, files in os.walk('./article'):\n",
    "    for name in tqdm(files):\n",
    "        names.append(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_d2v(text, lemma=True):\n",
    "    words = [x.lower().strip(string.punctuation+'»«–…') for x in word_tokenize(text)]\n",
    "    if lemma is True:\n",
    "        words = [morph.parse(x)[0].normal_form for x in words if x]    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_doc2vec(names):\n",
    "    '''\n",
    "    Train custome d2v model\n",
    "    :param names: pathes of the train data docs\n",
    "    :return: d2v model\n",
    "    '''\n",
    "    tagged_data = list()\n",
    "    for path in tqdm(names):\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            splitted = sp(text, 3)\n",
    "            for part in splitted:                \n",
    "                tagged_data.append(TaggedDocument(words=preprocessing_d2v(part, lemma=False), tags=[path]))\n",
    "    \n",
    "    model_d2v = Doc2Vec(vector_size=100, min_count=5, alpha=0.025, min_alpha=0.025, epochs=100, workers=4, dm=1, seed=42)\n",
    "    model_d2v.build_vocab(tagged_data)\n",
    "    print(len(model_d2v.wv.vocab))\n",
    "    \n",
    "    model_d2v.train(tagged_data, total_examples=model_d2v.corpus_count, epochs=model_d2v.epochs, report_delay=60)\n",
    "    return model_d2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_d2v = train_doc2vec(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = get_tmpfile(\"doc2vec_model_judic\")\n",
    "model_d2v.save(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# модель не обучилась за 1,5 суток, загружем готовую\n",
    "model_d2v = Doc2Vec.load('Doc2Vec_100s_1000e.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_d2v.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4\n",
    "Напишите функцию индексации для поиска через doc2vec. Она должна для каждого документа из корпуса получать вектор.    \n",
    "Все вектора надо сохранить, по формату советую json. При сохранении не забывайте, что вам надо сохранить не только вектор, но и опознователь текста, которому он принадлежит. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_d2v_vectors(words):\n",
    "    '''\n",
    "    Compute d2v vector for doc\n",
    "    :param words: list: lemmas of the doc for which to compute d2v vector\n",
    "    :return: list: d2v vector\n",
    "    '''\n",
    "    vec = model_d2v.infer_vector(words)\n",
    "    return vec \n",
    "\n",
    "def save_d2v_base(docs, idx):\n",
    "    \"\"\"\n",
    "    Save d2v vectors for all passed documents\n",
    "    :param texts: list: documents of collection \n",
    "    :param idx: list: names of documants from collection \n",
    "    :return: list: list of d2v vectors for input text\n",
    "    \"\"\"\n",
    "    base = list()\n",
    "    for i, doc in tqdm(enumerate(docs)):\n",
    "        parts = sp(doc, 3)\n",
    "        for p in parts:\n",
    "            lemmas = preprocessing(p)\n",
    "            base.append({'id': idx[i], 'text': p, 'vect': get_d2v_vectors(lemmas)})\n",
    "    return base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1384it [07:48,  2.95it/s]\n",
      "1384it [15:05,  1.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# сохраняем d2v базу для вопросов и ответов\n",
    "d2v_base_quest = save_d2v_base(questions, list(range(0, len(questions))))\n",
    "d2v_base_answ = save_d2v_base(answers, list(range(0, len(answers))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функция поиска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обратного индекса функцией поиска является Okapi BM25. Она у вас уже должна быть реализована."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция измерения близости между векторами нам пригодится:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similarity(v1, v2):\n",
    "    '''\n",
    "    Compute cosine similarity for 2 vectors\n",
    "    :param v1, v2: list: vectors\n",
    "    :return: float: vectors' similarity\n",
    "    '''\n",
    "    v1_norm = matutils.unitvec(np.array(v1))\n",
    "    v2_norm = matutils.unitvec(np.array(v2))\n",
    "    sim = np.dot(v1_norm, v2_norm)\n",
    "    if sim is not None:\n",
    "        return sim\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5\n",
    "Напишите функцию для поиска через word2vec и для поиска через doc2vec, которая по входящему запросу выдает отсортированную выдачу документов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_inv(query, questions, answers, inv_index) -> list:\n",
    "    \"\"\"\n",
    "    Search documents relative to query using inverted index algorithm.\n",
    "    :param query: str: input text\n",
    "    :param questions: list: all questions from corpus\n",
    "    :param answers: list: all answers from corpus\n",
    "    :param inv_index: list: questions inverted index\n",
    "    :return: list: 5 relative answers\n",
    "    \"\"\"\n",
    "    k1 = 2.0\n",
    "    b = 0.75\n",
    "    avgdl = np.mean(list(map(len, questions)))\n",
    "    N = len(questions)\n",
    "    \n",
    "    query_list = preprocessing(query)\n",
    "    scores = list()\n",
    "    \n",
    "    for i, doc in enumerate(questions):\n",
    "        score = 0\n",
    "        for word in query_list:\n",
    "            score += compute_sim(word, doc, inv_index, k1, b, avgdl, N)\n",
    "        scores.append([i, score])\n",
    "        \n",
    "    ranked = sorted(scores, key = lambda x: x[1], reverse=True)\n",
    "    result = [{'id': doc[0], 'text': answers[doc[0]]} for doc in ranked[:5]]\n",
    "\n",
    "    return result\n",
    "\n",
    "def search_w2v(query, w2v_base_quest, answers) -> list:\n",
    "    \"\"\"\n",
    "    Search documents relative to query using inverted w2v algorithm.\n",
    "    :param query: str: input text\n",
    "    :param w2v_base_quest: list: all questions' vectors from corpus\n",
    "    :param answers: list: all answers from corpus\n",
    "    :return: list: 5 relative answers\n",
    "    \"\"\"\n",
    "    \n",
    "    similarities = list()\n",
    "\n",
    "    for part in sp(query, 3):\n",
    "        lemmas = preprocessing(query)\n",
    "        vec = get_w2v_vectors(lemmas)\n",
    "    \n",
    "        for quest in w2v_base_quest:\n",
    "            s = similarity(vec, quest['vec'])\n",
    "            similarities.append({'id': quest['id'], 'sim': s})\n",
    "\n",
    "    ranked = sorted(similarities, key=lambda x: x['sim'], reverse=True)\n",
    "    result = [{'id': doc['id'], 'text': answers[doc['id']]} for doc in ranked[:5]]\n",
    "    \n",
    "    return result\n",
    "    \n",
    "def search_d2v(query, d2v_base_quest, answers) -> list:\n",
    "    \"\"\"\n",
    "    Search documents relative to query using inverted d2v algorithm.\n",
    "    :param query: str: input text\n",
    "    :param d2v_base_quest: list: all questions' vectors from corpus\n",
    "    :param answers: list: all answers from corpus\n",
    "    :return: list: 5 relative answers\n",
    "    \"\"\"\n",
    "    similarities = list()\n",
    "\n",
    "    for part in sp(query, 3):\n",
    "        lemmas = preprocessing(query)\n",
    "        vec = get_d2v_vectors(lemmas)\n",
    "    \n",
    "        for quest in d2v_base_quest:\n",
    "            s = similarity(vec, quest['vect'])\n",
    "            similarities.append({'id': quest['id'], 'sim': s})\n",
    "\n",
    "    ranked = sorted(similarities, key=lambda x: x['sim'], reverse=True)\n",
    "    result = [{'id': doc['id'], 'text': answers[doc['id']]} for doc in ranked[:5]]   \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После выполнения всех этих заданий ваш поисковик готов, поздравляю!                  \n",
    "Осталось завернуть все написанное в питон скрипт, и сделать общую функцию поиска гибким, чтобы мы могли искать как по обратному индексу, так и по word2vec, так и по doc2vec.          \n",
    "Сделать это можно очень просто через старый добрый ``` if ```, который будет дергать ту или иную функцию поиска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search(query, search_method):\n",
    "    if search_method == 'inverted_index':\n",
    "        search_result = search_inv(query, questions, answers, inv_q_base)\n",
    "    elif search_method == 'word2vec':\n",
    "        search_result = search_w2v(query, w2v_base_quest, answers)\n",
    "    elif search_method == 'doc2vec':\n",
    "        search_result = search_d2v(query, d2v_base_quest, answers)\n",
    "    else:\n",
    "        raise TypeError('unsupported search method')\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Оценка качества методов поиска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выдача считается \"успешной\", если среди первых пяти ответов, предложенных методом поиска есть настоящий ответ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_search_method(search_method):  \n",
    "    '''\n",
    "    Evaluate search method\n",
    "    :param search_method: search method to be evaluated\n",
    "    :return: float: share of the rigth answers\n",
    "    '''\n",
    "    success = 0\n",
    "    for i, q in tqdm(enumerate(questions)):\n",
    "        for res in search(q, search_method):\n",
    "            if i == res['id']:\n",
    "                success += 1\n",
    "                \n",
    "    return success / len(questions)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1384it [09:31,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec 1.778179190751445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1384it [17:31,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc2vec 1.9234104046242775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1384it [15:30,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inverted_index 0.9913294797687862\n"
     ]
    }
   ],
   "source": [
    "for method in ['word2vec', 'doc2vec', 'inverted_index']:\n",
    "    print(method, eval_search_method(method))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
