{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PageParser():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.url = 'https://www.avito.ru/rossiya/bilety_i_puteshestviya?p='\n",
    "        self.main = 'https://www.avito.ru'\n",
    "        self.headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) ' + \n",
    "                                  'AppleWebKit/537.73.11 (KHTML, like Gecko) Version/7.0.1 Safari/537.73.11'}\n",
    "        self.results_counter = 10100\n",
    "        self.links = list()\n",
    "    \n",
    "    def collect_links(self):\n",
    "        '''\n",
    "        Collect items links.\n",
    "        return: str: item metadata\n",
    "        '''\n",
    "        i = 1\n",
    "        while len(self.links) < self.results_counter:\n",
    "            try:\n",
    "                req = requests.get(self.url+str(i), headers=self.headers)\n",
    "                html = req.text\n",
    "                soup = BeautifulSoup(html, \"lxml\")\n",
    "                h3 = soup.find_all('h3')\n",
    "                for h in h3:\n",
    "                    self.links.append(h.find('a')['href'])\n",
    "                i += 1\n",
    "                time.sleep(random.randint(1,4))\n",
    "            except:\n",
    "                return            \n",
    "        return\n",
    "    \n",
    "    def parse_page(self, soup, url):\n",
    "        '''\n",
    "        Parse a page of an item and collect metadata.\n",
    "        soup: instance of BeautifulSoup class: item page structure\n",
    "        url: str: url of an item\n",
    "        return:\n",
    "        '''\n",
    "        text = url\n",
    "        info = list()\n",
    "        \n",
    "        info.append(soup.find('title'))\n",
    "        info.append(soup.find('div', attrs={'class':'title-info-metadata-item'}))\n",
    "        info.append(soup.find('div', attrs={'class':'item-description-text'}))\n",
    "        info.append(soup.find('span', attrs={'class':'js-item-price'}))\n",
    "        \n",
    "        location = soup.find('div', attrs={'class':'item-map-location'})\n",
    "        \n",
    "        info.append(location.find('span', attrs={'itemprop': 'name'}))\n",
    "        info.append(location.find(lambda tag: len(tag.attrs) == 0))\n",
    "        \n",
    "        for elem in info:\n",
    "            if elem is not None:\n",
    "                text += '\\n'\n",
    "                text += elem.text.strip(' \\n')        \n",
    "        return text\n",
    "    \n",
    "    def write_doc(self, text, url):\n",
    "        '''\n",
    "        Write down item's metadata as .txt file.\n",
    "        text: str: metadata to be wtitten\n",
    "        url: str: url of an item\n",
    "        return: \n",
    "        '''\n",
    "        if not os.path.exists('./avito_texts'):\n",
    "            os.makedirs('./avito_texts')\n",
    "        with open(r'./avito_texts/www.avito.ru%s.txt' %(url.replace('/', '=')), 'w', encoding='utf-8') as f:\n",
    "            f.write(text)\n",
    "        return\n",
    "        \n",
    "    def crowl(self):\n",
    "        '''\n",
    "        Iterate through all the items' pages, collect metadata and write it down.\n",
    "        return:\n",
    "        '''\n",
    "        for link in tqdm(self.links):\n",
    "            req = requests.get(self.main+link, headers=self.headers)\n",
    "            html = req.text\n",
    "            soup = BeautifulSoup(html, \"lxml\")\n",
    "            text = self.parse_page(soup, req.url)\n",
    "            self.write_doc(text, link)\n",
    "            time.sleep(random.randint(1,4))\n",
    "        return            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = PageParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.collect_links()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(parser.links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.crowl()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
